# Cases

### 500/405 error

**Situation:**  
Our team was alerted by another department that a significant number of 500 error were being generated by one of the services managed by our team, which was built on a customized Spring Boot framework. These errors were impacting the platform’s overall performance and needed immediate attention. Despite thorough code reviews and testing, the root cause wasn’t immediately apparent. The situation was critical because these errors were affecting the customer experience, and resolving the issue was essential.

**Task:**  
I was tasked with identifying the cause of these 500 error and implementing a solution that would correct the issue and prevent it from recurring. Given the severity of the situation, it was crucial to address the problem swiftly and ensure the service was handling errors appropriately.

**Action:**

1. **Initial Investigation:**  
   I began by thoroughly investigating the logs using the ELK Stack (Elasticsearch, Logstash, Kibana) to gather detailed information about the 500 errors. The logs provided some clues, but the root cause was still not evident. The errors were being flagged by another team because they originated from a service our team was responsible for. This prompted me to dive deeper into the specifics of how the service was handling requests.

2. **Identifying the Root Cause:**  
   After a detailed examination, I discovered that the 500 errors were being triggered by a customer who had inadvertently run a large number of scripts simultaneously, causing over 15,000 errors. I realized that the issue was rooted in our customized Spring Boot framework. Although we had tailored the framework to meet specific business requirements, we hadn’t explicitly defined a 405 error (Method Not Allowed) for cases where customers performed invalid operations. As a result, the system defaulted to returning a 500 error, which incorrectly indicated a server-side issue rather than a client-side misuse.

3. **Customizing the Framework:**  
   To resolve this, I extended our Spring Boot framework by implementing a custom error handler. I defined a specific exception that would correctly trigger a 405 error whenever an invalid operation was detected, ensuring that the system could accurately differentiate between server-side failures and client errors. This adjustment involved refining the exception handling logic within our service and ensuring it aligned with the overall architecture.

4. **Deployment and Monitoring:**  
   After implementing the changes, I deployed the updated framework and closely monitored the system using Prometheus and Grafana to confirm that the fix was effective. Additionally, I optimized our OpenSearch configurations to better manage high volumes of queries and reduce operational costs. This proactive monitoring ensured that the system was stable and that the errors had been appropriately reclassified.

**Result:**  
The implementation of the custom error handling resolved the issue, eliminating the 500 errors and ensuring that invalid customer operations now correctly triggered 405 errors. This improvement not only enhanced the stability and performance of the platform but also provided more accurate feedback to customers, reducing confusion and unnecessary strain on the system. The successful resolution of this issue reinforced the reliability of our service and demonstrated our team’s ability to quickly and effectively address complex technical challenges. The other team’s alert, which initially identified the problem, confirmed that our proactive response had fully mitigated the issue.

### Dynamic logging

**Situation:**  
In a critical project for a Spring Boot application integrated with the ELK stack, we were facing challenges with the existing static logging system. During periods of high traffic, we struggled to manage the volume of logs effectively, which hindered our ability to troubleshoot issues. The static log levels also meant that we couldn’t adjust the verbosity of the logs in real-time, which was crucial for debugging and performance monitoring. This logging issue was not originally in my area of responsibility, but I saw the impact it was having on the team and took ownership of solving it.

**Task:**  
I took it upon myself to redesign the logging system, ensuring it was flexible, dynamic, and scalable. My objective was to implement a solution that allowed the team to adjust log levels at runtime, enabling real-time troubleshooting without affecting application performance.

**Action:**

1. **Designing the Solution:** I researched different approaches to dynamic logging and decided on using Log4j2’s programmatic API. I designed a solution where log levels could be adjusted at runtime based on feature flags or manual overrides.
2. **Implementation:** I implemented the `DynamicLogLevelService` and integrated it with a feature flag service. This allowed the log level to be controlled by a configuration file or an environment variable, making it easy to change the log level across different environments (e.g., development, staging, production).

3. **Manual Override API:** I developed a REST API that allowed the team to manually adjust the log level in real-time. This was especially useful during incidents, where we needed to increase logging verbosity immediately to capture more details for troubleshooting.

4. **Collaboration and Documentation:** I documented the entire system, ensuring that the team understood how to use the new logging features. I also collaborated with the DevOps team to ensure the new logging system was integrated smoothly into our CI/CD pipeline and monitoring tools.

**Result:**  
The dynamic logging system became a core part of our application infrastructure. It improved the team's ability to troubleshoot issues in real-time, reduced the overhead of unnecessary logging, and empowered the DevOps team to take control of logging during critical incidents. This ownership not only solved the immediate logging issue but also created a long-term solution that could scale with the application. The success of this project led to its adoption in other services across the organization, further demonstrating the impact of taking ownership and delivering value.

### conflict with teammate

**Situation:**  
We were in the process of designing a new feature for our internal application that needed to handle a high volume of real-time transactions. Given the scalability and performance requirements, I strongly advocated for using **Amazon DynamoDB**. From my experience, DynamoDB's ability to handle high-velocity data with low latency and automatic scaling made it the ideal choice for our use case. However, one of my colleagues, who was leading the database management team, argued that **Amazon RDS** (a relational database) would be a more cost-effective solution, even though it required more manual management to achieve similar performance. This created a conflict between choosing a solution that prioritized scalability and ease of use (DynamoDB) versus one that emphasized cost-efficiency (RDS).

**Task:**  
As the advocate for DynamoDB, my task was to make a strong case for why it was the right choice for this feature. However, I also needed to carefully consider my colleague's concerns about cost and operational complexity, and ultimately come to a decision that balanced both performance and budget constraints.

**Action:**

1. **Evaluating DynamoDB's Benefits:**  
   I started by thoroughly researching DynamoDB's advantages for our use case. I highlighted its automatic scaling, ability to handle high write-throughput with low latency, and minimal operational overhead. Given the anticipated growth in transaction volume, I believed that DynamoDB's flexibility and performance would outweigh the higher cost.

2. **Analyzing RDS's Cost Efficiency:**  
   At the same time, I acknowledged my colleague's concerns about the cost difference. I worked closely with him to perform a detailed cost analysis for both DynamoDB and RDS over the projected lifespan of the feature. We considered factors like storage, read/write capacity, and the ongoing operational management required for RDS. The analysis showed that while DynamoDB was technically superior in performance, RDS could meet our performance needs with significant cost savings—about 30% less expensive over the first year.

3. **Open Discussion and Collaboration:**  
   I facilitated a discussion where both of us presented our findings to the team. I emphasized that DynamoDB's automatic scaling could reduce long-term operational effort and allow the team to focus on feature development rather than database management. However, my colleague effectively demonstrated that RDS could handle the current and foreseeable load with the right optimizations, and the cost savings could be better utilized in other areas of the project.

4. **Agreeing on a Pragmatic Approach:**  
   After considering all the data, I recognized that starting with RDS would allow us to balance both cost and performance effectively, especially in the early stages of the project. We agreed to proceed with RDS for the initial implementation, but with a plan in place to migrate to DynamoDB if our transaction volume grew beyond RDS’s capacity. This compromise allowed us to leverage RDS’s cost advantages while keeping the option to switch to DynamoDB if needed.

**Result:**  
By taking the time to thoroughly evaluate both options and engaging in open, constructive discussions, we reached a decision that benefited the project as a whole. RDS was implemented successfully, meeting the performance requirements and saving approximately 30% in database costs compared to DynamoDB. The feature was launched on time and within budget, and the flexibility of the plan allowed us to monitor the system’s growth and prepare for potential scaling challenges in the future. This experience taught me the importance of listening to different perspectives, even when I had a strong initial preference, and of being willing to adapt to the best solution for the business.

**Situation:**  
In one of my previous roles as a software developer, I was part of a team responsible for building a new feature for an eCommerce platform. The project involved integrating a complex payment processing system that needed to be both secure and scalable. However, the project began facing significant delays due to technical challenges, and the development efforts were not producing the desired results. Our team was struggling with performance issues related to database queries, and the project was at risk of missing critical deadlines.

Although I wasn’t the project lead, I saw that the project was in trouble and decided to take ownership of the performance issues, which were one of the main blockers for the team. I knew that resolving this problem could help get the project back on track.

**Task:**  
My task was to address the performance bottlenecks in the system, specifically focusing on optimizing database queries that were slowing down the payment processing flow. I wanted to ensure that the system could handle the required load while meeting the necessary security standards.

**Action:**

1. **Analyzing the Problem:**  
   I started by diving deep into the codebase to identify the specific areas where the performance issues were occurring. I worked closely with the database team to review the queries being executed and identified several inefficiencies in how data was being retrieved and processed. Some queries were running too frequently, fetching more data than necessary, and not taking full advantage of indexing.

2. **Optimizing the Queries:**  
   After identifying the bottlenecks, I took ownership of optimizing the database queries. I rewrote some of the most inefficient queries, reducing the amount of data being fetched and optimizing the indexing strategies. I also implemented caching for frequently accessed data, which significantly reduced the number of database calls.

3. **Collaborating with the Team:**  
   Although I wasn’t the lead, I communicated my findings and improvements to the rest of the team. I shared the optimizations I made and explained how they could improve the overall performance of the system. By proactively sharing these updates, I helped the team see progress in an area that had previously been a major obstacle. I also made myself available to assist other developers who were working on related parts of the system.

4. **Continuous Monitoring:**  
   I set up monitoring tools to track the performance of the optimized queries in real-time. This allowed us to quickly identify any new performance issues and address them before they became larger problems. I regularly checked the metrics and ensured that the system was performing as expected under load.

**Result:**  
As a result of taking ownership of the performance optimization, the payment processing system saw a significant improvement in response times—up to 40% faster. This helped unblock other parts of the project, allowing the team to meet the revised deadline. Although I wasn’t the official project lead, my proactive approach to solving the performance issues played a key role in turning the project around and ensuring its success. The project was delivered on time, and the client was satisfied with the outcome. This experience taught me that ownership isn’t just about titles or positions; it’s about stepping up when needed, solving problems, and driving results.

### Fixed a bug not belong to myself

**Situation:**  
While working as a developer on a critical feature for our eCommerce platform, a new version of our application had just been deployed to production. The deployment included a number of new functionalities, but shortly after it went live, I noticed a bug in one of the core features that impacted the checkout process. This bug wasn’t caught during testing and was causing some users to experience issues when completing their purchases. Even though the bug had already been deployed, and technically it wasn’t my responsibility to monitor production issues, I knew that the checkout flow was a key part of the user experience and decided to take ownership of the problem.

**Task:**  
My task was to identify the root cause of the bug, fix it as quickly as possible to minimize the impact on users, and ensure that the deployment was stable. Since this issue affected the live production environment, time was of the essence, and I needed to resolve it without causing further disruption.

**Action:**

1. **Investigating the Bug:**  
   I started by reproducing the issue in our staging environment, which closely mirrored production. After running several tests, I identified that the bug was caused by a faulty validation check in the backend code that wasn’t handling certain edge cases properly. This validation error was causing some transactions to fail during the checkout process.

2. **Quick Fix Implementation:**  
   Once I identified the root cause, I immediately worked on a fix. I modified the validation logic to correctly handle the edge cases that were causing the failures. After making the changes, I tested the fix thoroughly in both the staging and production environments to ensure it resolved the issue without introducing any new problems.

3. **Collaborating with the Team:**  
   Although I was primarily responsible for fixing the bug, I communicated with the DevOps team to arrange a quick patch deployment to production. I also informed the QA team so they could run additional tests on the production system to confirm that the issue was fully resolved. I made sure everyone involved was on the same page, and that the fix was deployed as quickly and smoothly as possible.

4. **Post-Deployment Monitoring:**  
   After the fix was deployed, I set up monitoring to track the checkout flow and ensure that the bug was no longer occurring. I kept a close eye on the error logs and worked with the customer support team to ensure that users who had experienced issues were informed that the problem had been resolved.

**Result:**  
The fix was successfully deployed with minimal downtime, and the checkout process returned to normal operations. By proactively taking ownership of the issue, I was able to prevent further user frustration and potential revenue loss for the company. My quick action and collaboration with the team ensured that the bug was resolved without causing further disruption. This experience reinforced the importance of owning problems, even after deployment, and acting quickly to fix them when they arise.

### Do not always build tools from scratch

**Situation:**  
Our team was tasked with developing a new feature for an internal tool that required a complex data processing pipeline. The pipeline needed to extract, transform, and load (ETL) data from various sources into our system. Initially, the team was planning to build this pipeline from scratch, which would have required a significant amount of time and effort, especially with tight deadlines. However, I realized that building everything from scratch wasn't necessary because other teams in our organization had built similar functionalities in their projects.

**Task:**  
My task was to find a way to simplify the development process by reducing the amount of redundant work. Specifically, I wanted to identify reusable components that could help us meet our objectives faster without compromising on quality. The goal was to save time and resources by reusing existing code instead of reinventing the wheel.

**Action:**

1. **Research and Exploration:**  
   I began by exploring repositories from other teams within our organization. I reached out to a few colleagues in other departments who had worked on similar projects to understand what reusable components they had developed. After reviewing several repositories, I found a robust ETL component that one of the other teams had already built. This component had been well-documented, thoroughly tested, and used in production, making it a reliable candidate for reuse.

2. **Evaluation and Adaptation:**  
   I carefully evaluated the reusable component to ensure it met our requirements. I ran tests to verify that it could handle our data sources and integrate with our existing architecture. While it wasn’t an exact fit, the core functionality was solid, and I saw that with some minor modifications, it could be adapted to meet our specific needs.

3. **Integration:**  
   After confirming its suitability, I integrated the component into our project. I made the necessary adjustments to the code and configuration to align it with our requirements, saving our team from having to build the ETL pipeline from scratch. The integration went smoothly, and since the component was already tested and optimized by the other team, we were confident in its stability.

4. **Documentation and Sharing:**  
   To ensure that our team and others could benefit from this in the future, I documented the integration process, detailing how the component could be reused for similar projects. I shared my findings and contributions with the original team and made our enhancements available to others within the organization.

**Result:**  
By reusing the existing ETL component from another team’s repository, I significantly simplified our development process. We were able to deliver the new feature well ahead of schedule, saving approximately 30% of the estimated development time. Additionally, by reusing a tested and stable component, we reduced the risk of introducing new bugs. This not only accelerated our timeline but also allowed the team to focus on refining the feature rather than building the pipeline from scratch. My approach demonstrated how thoughtful reuse and collaboration across teams can simplify complex tasks and drive efficiency across the organization.

### Monitoring data point

**Situation:**  
In my current role as a software engineer, I’ve often been assigned simple tasks, such as adding monitoring data points for various client types in our product. These tasks, while easy to implement, are time-consuming due to the strict release process in place. For example, one recent request from the product team was to add monitoring to report client type data to better understand the fulfillment rates across different platforms.

This task, while straightforward, involved more than just writing code. After the implementation, I had to go through rigorous approval processes, including drafting deployment plans, conducting load tests, and preparing reports. The entire procedure took two full days to complete, despite the actual code change taking only a few minutes.

**Task:**  
I needed to find a way to streamline this process and increase the overall efficiency of adding monitoring data points. The goal was to reduce the amount of time spent on repetitive tasks and avoid unnecessary delays, while still adhering to the strict release guidelines.

**Action:**  
To address this challenge, I designed and implemented a dynamic, configurable monitoring tool. My approach was to create a general-purpose component that would allow us to define monitoring data points via a centralized configuration system. Here’s how it worked:

1. **Modular Design:** I structured the monitoring component to accept any data fields, such as client type, channel type, price range, and more. The system could dynamically map these fields to their respective charts and data lines using a JSON configuration stored in our configuration management system (similar to Apollo).
2. **Dynamic Configuration:** Instead of hardcoding monitoring data points, I allowed the fields to be configurable. By utilizing a JSON configuration model, product and operations teams could dynamically add or modify the monitoring fields without requiring a full code release. This configuration change could be deployed and take effect in minutes.

3. **Scalability:** I built the system with scalability in mind, making it particularly useful for order fulfillment scenarios where many data points might need monitoring, including product types, discount types, client types, and so on. Over time, as the business evolved, the product team could easily enable monitoring for any of these fields without requiring my involvement.

**Result:**  
The introduction of this configurable monitoring tool significantly improved our efficiency. What previously took two days—due to the full deployment process—could now be accomplished in just a few minutes. By moving the control to a centralized configuration, we reduced the need for constant code changes and releases. This not only improved our operational agility but also empowered non-technical teams to adjust monitoring as needed.

### Pagination

**Situation:**  
In my current role, I faced a challenge when dealing with pagination in a high-traffic system. We initially used the simple `MySQL LIMIT OFFSET` approach for pagination, which worked fine for basic scenarios where the dataset was relatively small. However, as our system scaled and users needed to query larger datasets, especially during peak traffic, we started experiencing significant performance degradation. The deep pagination queries were slow, putting a strain on the database and causing a negative impact on overall system performance.

**Task:**  
I needed to find a more efficient pagination solution that could handle high traffic and large datasets without compromising on performance. The solution had to be flexible enough to accommodate different business needs, such as scenarios where we had complex search requirements or the need for real-time data.

**Action:**  
I researched and experimented with different pagination techniques, tailoring them to specific use cases within our system. Here’s the approach I took:

1. **Limit Offset Pagination:**  
   This method remained our go-to for simple scenarios where users needed to jump to specific pages and view total record counts. However, I recognized its limitations for deep pagination, so I restricted its use to lightweight queries where deep pagination was unlikely.

2. **Primary Key-Based Pagination:**  
   For scenarios involving deep pagination, I introduced primary key-based pagination. Instead of relying on offset, I modified the queries to filter results based on the last seen primary key. This reduced the load on the database by narrowing the search range with each successive query. This method significantly improved performance and reduced the risk of slow queries during peak traffic.

3. **HasMore Rolling Pagination:**  
   In scenarios where users didn’t need to know the total number of records, such as infinite scrolling interfaces, I implemented a rolling pagination method. Instead of fetching total records, I retrieved one extra record per page to determine if more data existed (`hasMore = true`). This avoided the need for `SELECT COUNT(*)` queries and optimized performance for user interfaces that didn't require total record counts.

4. **ElasticSearch Pagination:**  
   For complex search scenarios where performance was critical, I leveraged ElasticSearch. Although ElasticSearch also supports pagination similar to MySQL, I carefully tuned the pagination depth to avoid performance degradation. This approach proved useful for both B2B and B2C scenarios where users often performed complex searches over large datasets. I also ensured that ElasticSearch indexes were kept up-to-date with MySQL, understanding the trade-offs between real-time data and performance.

**Result:**  
By implementing these different pagination strategies, I was able to optimize our system for various use cases:

- The primary key-based pagination improved performance in high-traffic scenarios, reducing the load on our database.
- Rolling pagination enhanced the user experience in apps with infinite scrolling, while simultaneously decreasing the overall query cost.
- ElasticSearch-based pagination enabled us to handle complex search queries efficiently, especially for business-critical scenarios with large datasets.

Ultimately, this flexible approach allowed our system to scale effectively, improved performance, and provided a better user experience. It also prevented potential outages during peak traffic, aligning with Amazon’s “Customer Obsession” and “Bias for Action” principles by ensuring our customers had a seamless experience.
